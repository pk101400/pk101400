# âœ… 1. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
!git clone https://github.com/ZIZUN/korean-malicious-comments-dataset.git

import pandas as pd

file_path = "/content/korean-malicious-comments-dataset/Dataset.csv"
df = pd.read_csv(file_path, sep="\t") #íƒ­ êµ¬ë¶„ì ì£¼ì˜
df = df.dropna(subset=["content", "lable"])
print("ë°ì´í„° í¬ê¸°:", df.shape)
print(df.head())

#ê²°ì¸¡ì¹˜ ì œê±° ë° ë¼ë²¨ int ë³€í™˜
df = df.dropna(subset=["content", "lable"])
df["lable"] = df["lable"].astype(int)

# âœ… 2. ë°ì´í„° ë¶„í• 
from sklearn.model_selection import train_test_split

train_texts, val_texts, train_labels, val_labels = train_test_split(
    df["content"].tolist(),
    df["lable"].tolist(),
    test_size=0.2,
    random_state=42
)

print("í•™ìŠµ ìƒ˜í”Œ ìˆ˜:", len(train_texts))
print("ê²€ì¦ ìƒ˜í”Œ ìˆ˜:", len(val_texts))

# âœ… 3. í† í°í™” ë° ë°ì´í„°ì…‹ ìƒì„±
from transformers import AutoTokenizer
import torch

model_name = "monologg/koelectra-small-discriminator"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize_fn(texts):
    return tokenizer(texts, padding="max_length", truncation=True, max_length=128, return_tensors="pt")

train_encodings = tokenize_fn(train_texts)
val_encodings = tokenize_fn(val_texts)

class CommentDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = CommentDataset(train_encodings, train_labels)
val_dataset = CommentDataset(val_encodings, val_labels)





# âœ… 4. ëª¨ë¸ í•™ìŠµ
from torch.utils.data import DataLoader
from transformers import AutoModelForSequenceClassification
from torch.optim import AdamW
from tqdm import tqdm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)

#DataLoader ì„¤ì •
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)
#4, ì˜µí‹°ë§ˆì´ì € ì •ì˜
optimizer = AdamW(model.parameters(), lr=2e-5)

#5, í•™ìŠµ í•¨ìˆ˜ ì •ì˜
def train_epoch(model, dataloader, optimizer):
    model.train()
    losses = []
    loop = tqdm(dataloader, leave=False)
    for batch in loop:
        optimizer.zero_grad()
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        losses.append(loss.item())
        loss.backward()
        optimizer.step()
        loop.set_description(f"Training Loss: {loss.item():.4f}")
    return sum(losses) / len(losses)

#6, í‰ê°€ í•¨ìˆ˜ ì •ì˜
def evaluate(model, dataloader):
    model.eval()
    correct = 0
    total = 0
    loop = tqdm(dataloader, leave=False)
    with torch.no_grad():
        for batch in loop:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            outputs = model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            preds = torch.argmax(logits, dim=-1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)
            loop.set_description(f"Eval Acc: {correct/total:.4f}")
    return correct / total


#7, ì‹¤ì œ í•™ìŠµ ì‹¤í–‰(3 epoch)
for epoch in range(3):
    print(f"Epoch {epoch+1}/3")
    train_loss = train_epoch(model, train_loader, optimizer)
    val_acc = evaluate(model, val_loader)
    print(f"Train Loss: {train_loss:.4f} | Val Accuracy: {val_acc:.4f}")

def predict_comment(text, model, tokenizer):
    model.eval()
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=1).squeeze().tolist()
        pred = torch.argmax(torch.tensor(probs)).item()
    return pred, probs

import matplotlib.pyplot as plt

def plot_probs(probs, labels=["Malicious", "Normal"]):
    plt.figure(figsize=(6, 4))
    plt.bar(labels, probs)
    plt.ylim(0, 1)
    plt.title("Prediction Probabilities")
    plt.ylabel("Probability")
    plt.show()


# âœ… 5. ëª¨ë¸ ì €ì¥
model.save_pretrained("./trained_model")
tokenizer.save_pretrained("./trained_model")

# trained_model í´ë”ë¥¼ ZIPìœ¼ë¡œ ì••ì¶•í•´ì„œ trained_model.zip ì„ ìƒì„±
!zip -r trained_model.zip trained_model


#ì••ì¶• íŒŒì¼ì„ ë¡œì»¬ë¡œ ë‹¤ìš´ë¡œë“œ
from google.colab import files
files.download("trained_model.zip")


requirements_content = """\
streamlit==1.34.0
transformers==4.41.2
torch
matplotlib
sentencepiece
"""

with open("requirements.txt", "w") as f:
    f.write(requirements_content)


app_py_content = '''
import streamlit as st
import torch
import matplotlib.pyplot as plt
from transformers import AutoModelForSequenceClassification, AutoTokenizer

@st.cache_resource
def load_model_tokenizer():
    model = AutoModelForSequenceClassification.from_pretrained("trained_model")
    tokenizer = AutoTokenizer.from_pretrained("trained_model")
    model.eval()
    return model, tokenizer

model, tokenizer = load_model_tokenizer()

sexual_terms = ["ì –", "ë²—ì–´", "ê°€ìŠ´", "ì—‰ë©ì´", "ì•¼ì§¤", "ì•¼ë™", "ì„¹", "ì„±í¬ë¡±", "ë¬¼ë¹¼ê²Œ", "ëª¸ë§¤", "ì•¼í•´", "ì˜¤ë¹ ê°€", "ììœ„", "ë”¸ì³", "ì•¼í•œ", "ã……ã……", "ã…ˆã…‡", "ã…‚ã…ˆ"]
positive_terms = ["ìµœê³ ", "ë©‹ì ¸", "ëŒ€ë°•", "ì‚¬ë‘", "ê°ë™", "ì§±", "ì¸ì •", "êµ¿", "ê°íƒ„", "êµ‰ì¥í•´", "ì˜ìƒê²¼", "ì˜ˆì˜", "ì˜í–ˆì–´", "ë ˆì „ë“œ", "ì²œì¬", "ì¢‹ì•„"]

def contains_sexual_harassment(text):
    return any(term in text.lower() for term in sexual_terms)

def is_positive_context(text):
    return any(term in text.lower() for term in positive_terms)

def final_decision(text, label):
    if contains_sexual_harassment(text):
        return "ì•…ì„± (ì„±í¬ë¡±)"
    elif label == 0 and is_positive_context(text):
        return "ì •ìƒ (ìš•ì„¤ + ì¹­ì°¬)"
    elif label == 0:
        return "ì•…ì„±"
    else:
        return "ì •ìƒ"

def predict_proba(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=-1).squeeze().tolist()
    return probs

st.title("ğŸ›¡ï¸ ì•…ì„± ëŒ“ê¸€ íƒì§€ê¸° ë°ëª¨")
comment = st.text_area("ğŸ“ ëŒ“ê¸€ì„ ì…ë ¥í•˜ì„¸ìš”:", "")

if comment:
    probs = predict_proba(comment)
    label = int(torch.argmax(torch.tensor(probs)))
    result = final_decision(comment, label)

    st.markdown("---")
    st.subheader("ğŸ“Š ì˜ˆì¸¡ ê²°ê³¼")
    st.write(f"ğŸ‘‰ **ì˜ˆì¸¡ ê²°ê³¼:** {result}")
    st.write(f"- ëª¨ë¸ ë¼ë²¨: {'ì•…ì„±' if label == 0 else 'ì •ìƒ'} (í™•ë¥  {probs[label]*100:.1f}%)")
    if result == "ì •ìƒ (ìš•ì„¤ + ì¹­ì°¬)":
        st.write("- í›„ì²˜ë¦¬ íŒë‹¨: **ì •ìƒ**")
        st.write("- ìœ„í—˜ë„ ë ˆë²¨: ğŸŸ¡ **ì£¼ì˜ í•„ìš”**")
    elif "ì•…ì„±" in result:
        st.write("- ìœ„í—˜ë„ ë ˆë²¨: ğŸ”´ **ìœ„í—˜**")
    else:
        st.write("- ìœ„í—˜ë„ ë ˆë²¨: ğŸ”µ **ì •ìƒ**")

    st.subheader("ğŸ“ˆ ë¶„ë¥˜ í™•ë¥  ì‹œê°í™”")
    fig, ax = plt.subplots()
    ax.bar(["Malicious", "Normal"], probs, color=["red", "blue"])
    ax.set_ylim([0, 1])
    ax.set_ylabel("Probability")
    st.pyplot(fig)

    st.markdown("---")
    st.subheader("ğŸ“Œ í™œìš© ì˜ˆì‹œ")
    st.markdown("- í•™êµ ê²Œì‹œíŒ ìë™ í•„í„°ë§ ì‹œìŠ¤í…œ")
    st.markdown("- ì—°ì˜ˆì¸ ê¸°ì‚¬ ëŒ“ê¸€ ëª¨ë‹ˆí„°ë§")
    st.markdown("- ì»¤ë®¤ë‹ˆí‹° ìš´ì˜ììš© ì‹¤ì‹œê°„ ê´€ë¦¬ íˆ´")
'''

with open("app.py", "w", encoding="utf-8") as f:
    f.write(app_py_content)


from google.colab import files
files.download("requirements.txt")
files.download("app.py")
